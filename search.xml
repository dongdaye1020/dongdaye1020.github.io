<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[集成学习 模型融合方法简介]]></title>
    <url>%2F2018%2F06%2F29%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[前言：以下内容主要是本人在学习台大林轩田机器学习技法课程中模型融合部分以及《统计学习方法》相关内容的学习心得，且不涉及公式推导和证明部分。 1.集成学习的动机与框架（1）为什么要用集成学习？集成学习是通过构建和组合多个较弱的学习器，得到一个较强的模型，是因为这样做有两个优点：第一，cure underfitting（降低偏差）：有助于防止欠拟合，它把所有弱的学习器融合起来，能达到提取组合特征的功能（每一个弱的学习器相当于组合提取了一次特征），起到了feature transform的作用，可以得到较为复杂的模型;第二，cure overfitting（降低方差）：有助于防止过拟合，把所有学到的学习器进行组合，将可能过拟合与欠拟合的学习器进行中和，容易得到一个中庸的模型，防止过拟合。 （2）集成学习的框架简介按照集成学习组合策略方式的不同，可以将集成学习分为三类：uniform、non-uniform和conditional。（我们可以简单地理解为每人一票且同样重要、每人一票但要加权、看情况选择部分人加权投票） 从集成学习的定义可以看出，我们需要两步，一是得到不同的学习器g(t)，二是使用策略组合得到的g(t)。我们称第二步的过程为blending，在g(t)已知的情况下，blending对应的分类如下： aggregation blending uniform voting/averaging non-uniform linear conditional stacking 可以证明使用voting的blending策略组合g(t)可以得到更好的模型（主要是从期望角度证明了，平均组合后的G(t)可以降低模型的方差，能得到更加稳定的模型），而linear策略的本质则是通过线性变换达到feature transform的作用，stacking则是非线性变换（此策略可以得到更为复杂的模型，所以应该注意防止过拟合）。 那么在g(t)未知的情况，如何才能得到不同的g(t)？通常情况下，可以选取模型、设置不同的参数、不同的样本数据等方法获得。但通常情况下，我们只有一份数据集，此时可以借鉴统计学中boostrap思想，利用已知的样本数据，通过放回的重抽样方法，获得不同的样本数据集。 回到集成学习的具体实现上，我们需要获得不同的g(t)，并使用策略进行组合，我们将这整个过程成为learing。使用不同blending策略，对应着不同的learing方法。 接下来可以通过三种代表性的learing方法，来实现集成学习。 2.集成学习的初阶实现 aggregation blending learing uniform voting/averaging bagging non-uniform linear boosting conditional stacking Decision Tree （1）bagging正如第一部分所讲的，bagging其实就是利用boostrap方法对数据集进行重抽样，获得多个数据集，以多个数据集为基础，训练得到多个学习器g(t)，最后通过平均化形成最终的模型，直观地讲，在回归问题上是对每个最终结果求平均，在分类问题上则是多数表决原则。由于只使用bagging一直方法的情况很少，故此处不再赘述。 （2）boostingboosting，顾名思义是通过迭代的方法不断提升组合模型的效果，也就是加法模型，最常见的就是adaboost。算法的一般步骤包括：对于1,2,3,~,T轮迭代过程中,选择损失函数以及弱分类器g(t)后：a.寻找能使损失函数最快降低的g(t)，及其对应的系数α；b.更新加法模型后，再次进行a步骤。 adaboost是用来解决二分类问题的，其损失函数为指数损失函数。通过每一轮模型的误分率来调整样本权重以获得下一步的新g(t)，并以此为基础来计算该轮g(t)的系数。 按照李航老师的解释，可以将adaboost看成是损失函数是指数损失函数的前向分布算法(证明的话是先固定模型权重，证每轮更新的g(t)相等，然后再带入原式中，通过求偏导数，计算权重值)。 按照林轩田老师的解释，可以将adaboost看成损失函数是指数损失、处理二分类问题的Gradient Boosting算法，这样的好处是可以方便引出下面的GBDT算法，此处的证明有两个个要点 ： a. 第t+1轮的所有样本的权重和Z与前t轮模型的损失函数是相等的，此时可以将其看成是类似与svm中的margin，所以可以理解为每轮寻找适当的g(t)，使得Z变小（这里，李航老师给出了证明：adaboost的训练误差是小于所有轮权重和的乘积的，每轮的优化方向也是使得当前轮的Z最小）； b. 利用梯度下降的思想，把函数看成向量，也就是说再向量空间里，先找到最好的更新函数，也就是该函数的负梯度方向，再寻找最优的下降步长； 此外，对于adaboost需要注意的是：当每轮的g(t)的误分概率小于0.5时，其训练误差可以以指数速率下降。 这里李航老师根据训练误差上界继续推导即可得证，而林轩田老师则是以VC bound角度来看，经过O(logN)轮次迭代，训练误差将会减小到0的程度。 缺点： 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。 优点： Adaboost作为分类器时，分类精度很高，作为简单的二元分类器时，构造简单，结果可理解。不容易发生过拟合 可以看出，boosting方法主要是减少模型的偏差。 （3）Decision Tree为什么说Decision Tree对应的conditional条件的aggregation？ 简单来讲，对于任意一颗决策树，把每条路径看成是Qt(x)，与之对应的叶子节点看成是Gt(x)，那么该决策树就可以表示成的conditional条件的aggregation，具体如下： 缺点： 预测结果缺乏平滑性，不适合处理高维稀疏数据 优点： 可解释性强、可处理混合类型特征、 具体伸缩不变性（不用归一化特征）、 有特征组合的作用、可自然地处理缺失值、对异常点鲁棒、有特征选择作用、可扩展性强，容易并行 常见的决策树算法有ID３、C４.５和CART,具体的算法细节和注意点在另一篇文章中可以看到。（点此处） 3.集成学习的高阶实现从上面的讨论可以看出bagging主要是减少模型的方差，而boosting主要是减少模型的偏差，尝试将多个模型、策略再次组合，可以得到更加强健的模型。 （1）Random Forest将bagging与Decision Tree结合，就得到了Random Forest。此处的bagging是bagging in everywhere：一是在抽取数据时用了Bagging(这里也导致了其另一个优良性：无需额外交叉验证)；二是在构建每一个决策树时，随机抽取一部分特征，起到了特征转换的作用；三可以用数组p进行线性组合，来保持特征多样性。 两个特性：第一，无需额外的交叉验证，用oob样本对应的决策树的平均预测值近似地代表最终预测值，通过该值的误差平方和近似地表示，最后模型的预测能力。可以证明oob（out of bag）样本占比约为1/e; 第二，通过random test进行特诊筛选（random test的思路是对于某个特征，用随机值代替原值，比较代替前后的模型表现，若无影响，则可以忽略），一般有两种方式进行random test，一时用已知的随机分布例如高斯分布去代替，二是将该特征的原始值进行打乱重排，也就是permutation test（随机排序测试）。后者显然更科学，因为没有改变原始分布情况。random forest为了减少计算量，直接对比oob样本的ermutation test后的模型表现，做出特征筛选。 缺点： 随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。 对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。 优点： 随机森林算法有很强的抗干扰能力：对于不平衡数据集来说，随机森林可以平衡误差。如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。 随机森林抗过拟合能力比较强：对generlization error(泛化误差)使用的是无偏估计模型。 （2）GBDT、XGboost、LightGBM以及CatboostGBDT具体得算法步骤如下： 取g(t)为cart决策回归树时，就是GBDT了。当损失函数为误差平方和时，解决的时回归问题，当损失函数时Log-Likehood 是，解决的是分类问题。 注意：每次更新的g(t)是损失函数的负梯度方向（利用了损失函数一阶导数信息）。 缺点： boosting是个串行的过程，所以并行麻烦，需要考虑上下树之间的联系，计算复杂度大，不适合高维稀疏特征 优点： 能适应多种损失函数，不需要做特征的归一化，可以自动选择特征，模型可解释性好等等 XGboostXGboost可以看作时GBDT的高效的实现版本，相比于GBDT其损失函数中添加了正则项：对每棵树的复杂的进行了惩罚，具体如下所示： 此外，在优化目标函数时，使用了其二阶导数的信息，也就是牛顿法求最小（二阶泰勒展开）。 关于XGboostd的详细问题，例如树结构确定方法（贪心法）、打分函数、树节点分裂方法、稀疏值处理、行采样、列采样、学习率（Shrinkage）、 支持自定义损失函数（需二阶可导）等，可在这里查看（插个眼） 注意：XGBoost无法单独处理分类特征，它是基于CART的，所以只接收数值。在把分类数据提供给算法前，我们先得执行各种编码，如标签编码、均值编码、独热编码等。 LightGBM与XGBoost相比，速度更快，内存占用更低。改进的地方：利用直方图算法减小内存占用和计算增益的计算量、建树过程优化Leaf-wise（基于梯度的单侧采样（GOSS）不依赖剪枝）、并行优化（特征并行【避免广播instance indices，减小网络通信量，但单个worker运算代价高】、数据并行【直方图算法，减少通信量】） catboost和LightGBM类似，更适合处理含有分类特征的数据，CatBoost在分类变量索引方面具有相当的灵活性，它可以用在各种统计上的分类特征和数值特征的组合将分类值编码成数字（one_hot_max_size：如果feature包含的不同值的数目超过了指定值，将feature转化为float）。LightGBM也可以通过输入特征名称来处理分类特征。它无需进行独热编码（one-hot coding），而是使用一种特殊的算法直接查找分类特征的拆分值，最后不仅效果相似，而且速度更快。 以下列出三者部分参数的区别： （来源于互联网，图侵删） 以上是本文的全部内容，学的都是些皮毛，肯定存在很多错误，欢迎讨论指教！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>boosting</tag>
        <tag>bagging</tag>
      </tags>
  </entry>
</search>
